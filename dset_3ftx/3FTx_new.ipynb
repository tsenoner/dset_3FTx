{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ToDo\n",
    "- [x] get representatives in data\n",
    "- [x] ideam is the same as previous entry\n",
    "- [x] get prot seqs from UniProt (API call)\n",
    "- [x] clean up Pevious FASTA file\n",
    "- [x] combine results with previous FASTA\n",
    "- [x] check for duplicate sequences\n",
    "- [x] extract UniProtID from old CSV and put in new column\n",
    "- [ ] check for duplicates between both dsets\n",
    "- [ ] merge CSV files\n",
    "- [ ] create feature file for protspace3D\n",
    "- [ ] show results in ProtSpace3D (separate those by others)\n",
    "- [ ] Split by this group\n",
    "- [ ] highlight representative\n",
    "- [ ] only look at french data separatelly"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [ ] check if possible to get whole sequence of genomic sequences (without UniProt entry)\n",
    "- [ ] retrieve names\n",
    "- [ ] merge activity names\n",
    "- [ ] add french data\n",
    "- [ ] check if cluster\n",
    "- [ ] which dataset clusters best? full, "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- NCBI selection by acc_id ([AccID list](https://www.ncbi.nlm.nih.gov/books/NBK25497/table/chapter2.T._entrez_unique_identifiers_ui/?report=objectonly))\n",
    "  - entries comming from WGS, which are they?\n",
    "  - ignore predicted entries, or add but label? -> XM_ XR_ XP_\n",
    "    - maybe in the meantime there exists a match in uniprot?\n",
    "  - Which entries always to keep?\n",
    "  - separation by division? https://www.ncbi.nlm.nih.gov/genbank/samplerecord/#GenBankDivisionA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- how to decide which sequenc to keep?\n",
    "- what could a kind of quality controll be?\n",
    "  - Exclude if protein evidence is at homology or predicted level.\n",
    "  - Include if at protein or transcription level\n",
    "  - Exclude if it comes from EST, DNA, CDNA?\n",
    "  - only take mRNA\n",
    "  - sequences which are experimenally verified to be translated (uniprot: at protein level/transcript level)\n",
    "  - What would be a good decision?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <script type=\"application/javascript\" id=\"jupyter_black\">\n",
       "                (function() {\n",
       "                    if (window.IPython === undefined) {\n",
       "                        return\n",
       "                    }\n",
       "                    var msg = \"WARNING: it looks like you might have loaded \" +\n",
       "                        \"jupyter_black in a non-lab notebook with \" +\n",
       "                        \"`is_lab=True`. Please double check, and if \" +\n",
       "                        \"loading with `%load_ext` please review the README!\"\n",
       "                    console.log(msg)\n",
       "                    alert(msg)\n",
       "                })()\n",
       "                </script>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import jupyter_black\n",
    "\n",
    "jupyter_black.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original - 954 entries: 623 UniProt IDs; 1 RefSeq IDs; 47 GenBank IDs identified.\n",
      "- 127 full sequences information added by genomic supported alignment\n",
      "Zhang - 1003 entries: 274 UniProt IDs; 85 RefSeq IDs; 596 GenBank IDs identified.\n",
      "Ritu - 119 entries: 8 UniProt IDs, 111 GI numbers identified\n",
      "French - 39 entries: 39 UniProt IDs, identified\n",
      "UniProt - 617 entries: 617 UniProt IDs identified\n",
      "\n",
      "\n",
      "778 duplicate sequences were merged.\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "import dset_3FTx\n",
    "import uniprot_helper\n",
    "import ncbi_helper\n",
    "\n",
    "# importlib.reload(dset_3FTx)\n",
    "# importlib.reload(uniprot_helper)\n",
    "\n",
    "# pd.io.clipboards.to_clipboard(df.to_markdown(), excel=False)\n",
    "\n",
    "# --- PATHS ---\n",
    "base = Path(\"../data\")\n",
    "out_dir = base / \"protspace\"\n",
    "raw = base / \"raw\"\n",
    "helpers = base / \"helpers\"\n",
    "\n",
    "csv_in = raw / \"Ivan_3FTx.csv\"\n",
    "fasta_in = raw / \"3and6_new-2.fasta\"\n",
    "genomic_fasta = raw / \"Translation of 156 sequences.fasta\"\n",
    "zhang_fasta = raw / \"BungarusMulticinctus.fasta\"\n",
    "ritu_csv = raw / \"drysdalia.csv\"\n",
    "french_excel = raw / \"french_data.xls\"\n",
    "uniprot_uids_files = [raw / \"dashev_uniprot.txt\", raw / \"snake_3FTx_sp.txt\"]\n",
    "\n",
    "ncbi_dir = base / \"ncbi_entries\"\n",
    "uniprot_dir = base / \"uniprot_entries\"\n",
    "blast_dir = base / \"blast_out\"\n",
    "# nuc_dir = base / \"gi_number\"\n",
    "taxon_mapper_file = helpers / \"taxon_mapper.csv\"\n",
    "# gi2accid_file = helpers / \"gi2accid.json\"\n",
    "\n",
    "fasta_out = out_dir / \"3FTx.fasta\"\n",
    "csv_out = out_dir / \"3FTx.csv\"\n",
    "\n",
    "# --- MAIN ---\n",
    "uniprot_collector = uniprot_helper.UniProtDataGatherer(uniprot_dir=uniprot_dir)\n",
    "ncbi_collector = ncbi_helper.NcbiDataGatherer(ncbi_dir=ncbi_dir)\n",
    "\n",
    "df_original = dset_3FTx.OriginalDset(\n",
    "    csv_path=csv_in, fasta_path=fasta_in, genomic_fasta_path=genomic_fasta\n",
    ").df\n",
    "df_zhang = dset_3FTx.ZhangDset(fasta_path=zhang_fasta).df\n",
    "df_ritu = dset_3FTx.RituDset(csv_path=ritu_csv).df\n",
    "df_french = dset_3FTx.FrenchDset(excel_path=french_excel).df\n",
    "df_uniprot = dset_3FTx.parse_uniprot_ids_file(uniprot_uids_files=uniprot_uids_files)\n",
    "df = pd.concat(\n",
    "    [df_original, df_french, df_zhang, df_ritu, df_uniprot], ignore_index=True\n",
    ")\n",
    "df = dset_3FTx.map_ids2uniprot(df=df)\n",
    "# 28 of original mature_seq have missing ends or no UniProt entry\n",
    "df = dset_3FTx.get_uniprot_metadata(df=df)\n",
    "df = dset_3FTx.get_ncbi_metadata(df=df)\n",
    "df = df.dropna(subset=\"species\")\n",
    "df = dset_3FTx.add_taxon_id(df=df, taxon_mapper_file=taxon_mapper_file)\n",
    "# TODO: run BLASTp to find UniProt entries\n",
    "#       ignore entries that do already have an acession number\n",
    "# df = dset_3FTx.run_blast(\n",
    "#     df=df, blast_dir=blast_dir, uniprot_collector=uniprot_collector\n",
    "# )\n",
    "df = dset_3FTx.remove_low_quality_entries(df=df)\n",
    "df = dset_3FTx.manual_curation(df=df)\n",
    "dset_3FTx.save_data(df=df, csv_file=csv_out, fasta_file=fasta_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyfaidx import Fasta\n",
    "\n",
    "f = Fasta(out_dir / \"3FTx_mature.fasta\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_df(df):\n",
    "    df.index = df.index.fillna(\"NA\")\n",
    "    pd.io.clipboards.to_clipboard(df.to_markdown(), excel=False)\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "original             802\n",
       "paper_zhang          669\n",
       "genomic              152\n",
       "paper_RituChandna    119\n",
       "Name: data_origin, dtype: int64"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"data_origin\"].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SP    701\n",
      "NA    621\n",
      "TR    267\n",
      "NA    153\n",
      "Name: db, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Entries found in UniProt\n",
    "res = df[\"db\"].value_counts(dropna=False)\n",
    "copy_df(df=res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0    1303\n",
      "NA      439\n",
      "Name: acc_id, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# number of UniProt accession IDs with a 100% sequence match\n",
    "res = df[\"acc_id\"].str.split(\",\").str.len().value_counts(dropna=False)\n",
    "copy_df(df=res)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get UniProt full sequence\n",
    "\n",
    "- [ ] differentiate between chain and signal sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "df[df[\"name\"].isna()]\n",
    "# df.loc[df[\"name\"] == \"-\", \"name\"] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "uniprot_dir = Path(\"uniprot_entry\")\n",
    "\n",
    "data = []\n",
    "for idx, row in df.iterrows():\n",
    "    if row[\"acc_id\"] is None:\n",
    "        if row[\"name\"] == \"-\":\n",
    "            row[\"name\"] = np.nan\n",
    "        data.append(row)\n",
    "    else:\n",
    "        acc_ids = row[\"acc_id\"].split(\",\")\n",
    "        for acc_id in acc_ids:\n",
    "            row = row.copy()\n",
    "            # get data TODO: refractor\n",
    "            json_path = uniprot_dir / f\"{acc_id}.json\"\n",
    "            if json_path.is_file():\n",
    "                with open(json_path, \"r\") as json_handler:\n",
    "                    json_data = json.load(json_handler)\n",
    "            else:\n",
    "                response = uniprot_helper.get_uniprot_entry(\n",
    "                    acc_id=acc_id, format_type=\"json\"\n",
    "                )\n",
    "                json_data = response.json()\n",
    "                with open(json_path, \"w\") as json_handler:\n",
    "                    json.dump(json_data, fp=json_handler, indent=4)\n",
    "\n",
    "            # check if a full sequence exists\n",
    "            # - features Signal + Chain\n",
    "            # - different from existing sequence\n",
    "            # - start with residue M\n",
    "            features = set()\n",
    "            if \"features\" in json_data:\n",
    "                for feature in json_data[\"features\"]:\n",
    "                    feature_type = feature[\"type\"]\n",
    "                    features.add(feature_type)\n",
    "            if set([\"Signal\", \"Chain\"]).issubset(features):\n",
    "                json_seq = json_data[\"sequence\"][\"value\"]\n",
    "                if (json_seq != row[\"seq\"]) and (json_seq.startswith(\"M\")):\n",
    "                    row[\"full_seq\"] = json_data[\"sequence\"][\"value\"]\n",
    "\n",
    "            # TODO: get the mature sequence\n",
    "            # if \"Propeptide\" in features:\n",
    "            #     print(acc_id, features)\n",
    "\n",
    "            \n",
    "\n",
    "            # add UniProt info to entries\n",
    "            row[\"name\"] = name\n",
    "            row[\"acc_id\"] = acc_id\n",
    "            row[\"entry\"] = json_data[\"uniProtkbId\"]\n",
    "            row[\"prot_evi\"] = json_data[\"proteinExistence\"]\n",
    "            row[\"annot_score\"] = json_data[\"annotationScore\"]\n",
    "            data.append(row)\n",
    "new_df = pd.DataFrame(data)\n",
    "new_df = new_df.drop_duplicates(subset=[\"seq\", \"full_seq\", \"taxon_id\"])\n",
    "# make unique ID\n",
    "new_df[\"acc_id\"] = new_df[\"acc_id\"].fillna(\"\")\n",
    "new_df[\"acc_id\"] = new_df[\"acc_id\"] + \"__\" + new_df[\"fasta_id\"]\n",
    "new_df[\"genus\"] = new_df[\"species\"].str.split().str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_names = {\n",
    "    \"Adrenergic toxin\": [\"Adrenergic toxin\"],\n",
    "    \"Alpha-bungarotoxin\": [\"Alpha-bungarotoxin\"],\n",
    "    \"Alpha-elapitoxin\": [\"Alpha-elapitoxin\"],\n",
    "    \"Alpha-neurotoxin\": [\"Alpha-neurotoxin\"],\n",
    "    \"Beta-cardiotoxin\": [\"Beta-cardiotoxin\"],\n",
    "    \"Candiduxin\": [\"Candiduxin\"],\n",
    "    \"Cardiotoxin\": [\"Cardiotoxin\"],\n",
    "    \"Cobrotoxin\": [\"Cobrotoxin\"],\n",
    "    \"Cytotoxin\": [\"Cytotoxin\"],\n",
    "    \"Dendroaspin\": [\"Dendroaspin\"],\n",
    "    \"Erabutoxin\": [\"Erabutoxin\"],\n",
    "    \"Fasciculin\": [\"Fasciculin\"],\n",
    "    \"Frontoxin\": [\"Frontoxin\"],\n",
    "    \"Irditoxin\": [\"Irditoxin\"],\n",
    "    \"Kappa\": [\"Kappa\"],\n",
    "    \"Long neurotoxin\": [\"Long neurotoxin\"],\n",
    "    \"Ly6\": [\"Ly6\", \"Ly-6\", \"Lymphocyte antigen 6\", \"Prostate stem cell antigen\"],\n",
    "    \"Mambalgin\": [\"Mambalgin\"],\n",
    "    \"Micrurotoxin\": [\"Micrurotoxin\"],\n",
    "    \"Mipartoxin\": [\"Mipartoxin\"],\n",
    "    \"Muscarinic\": [\"Muscarinic\"],\n",
    "    \"Neurotoxin\": [\"Neurotoxin\"],\n",
    "    \"Probable weak neurotoxin\": [\"Probable weak neurotoxin\"],\n",
    "    \"Pseudonajatoxin\": [\"Pseudonajatoxin\"],\n",
    "    \"Short neurotoxin\": [\"Short neurotoxin\"],\n",
    "    \"Synergistic-like venom protein\": [\"Synergistic-like venom protein\"],\n",
    "    \"3FTx\": [\n",
    "        \"Three finger toxin\",\n",
    "        \"Three-finger toxin\",\n",
    "        \"Toxin 3FTx\",\n",
    "        \"Toxin\",\n",
    "    ],\n",
    "    \"Toxin_TOLIP\": [\"Toxin_TOLIP\"],\n",
    "    \"Three-finger hemachatoxin\": [\"Three-finger hemachatoxin\"],\n",
    "    \"Weak neurotoxin\": [\"Weak neurotoxin\"],\n",
    "    \"Weak toxin\": [\"Weak toxin\"],\n",
    "}\n",
    "\n",
    "# extract protein activity from protein name\n",
    "new_df[\"new_name\"] = new_df[\"name\"]\n",
    "for new_name, old_name_lst in new_names.items():\n",
    "    for old_name in old_name_lst:\n",
    "        new_df.loc[\n",
    "            new_df[\"name\"].str.contains(old_name, na=False), \"new_name\"\n",
    "        ] = new_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data_origin\n",
       "genomic        29\n",
       "dtype: int64"
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.loc[new_df[\"new_name\"].isin([\"Ly6\"]), [\"data_origin\"]].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_data = new_df.pop(\"acc_id\")\n",
    "new_df.insert(loc=0, column=\"acc_id\", value=col_data)\n",
    "\n",
    "csv_file = Path(\"../3FTx/3FTx.csv\")\n",
    "fasta_file = Path(\"../3FTx/3FTx.fasta\")\n",
    "fasta_file_full_seq = Path(\"../3FTx/3FTx_full.fasta\")\n",
    "new_df.to_csv(csv_file, index=False)\n",
    "with open(fasta_file, \"w\") as fasta_handler:\n",
    "    for _, row in new_df.iterrows():\n",
    "        fasta_handler.write(f\">{row['acc_id']}\\n\")\n",
    "        fasta_handler.write(f\"{row['seq']}\\n\")\n",
    "with open(fasta_file_full_seq, \"w\") as fasta_handler:\n",
    "    for _, row in new_df.dropna(subset=\"full_seq\").iterrows():\n",
    "        fasta_handler.write(f\">{row['acc_id']}\\n\")\n",
    "        fasta_handler.write(f\"{row['full_seq']}\\n\")\n",
    "\n",
    "\n",
    "# import h5py\n",
    "\n",
    "# esm = \"../3FTx_full/3FTx_esm2.h5\"\n",
    "# esm_h5 = \"../3FTx_full/3FTx_esm2_.h5\"\n",
    "# csv_ = \"../3FTx_full/3FTx_.csv\"\n",
    "# id_tracker = dict()\n",
    "# with h5py.File(esm, \"r\") as hdf, h5py.File(esm_h5, \"w\") as hdf_new:\n",
    "#     for uid, emb in hdf.items():\n",
    "#         fasta_id = new_df.loc[new_df[\"acc_id\"] == uid, \"fasta_id\"].values[0]\n",
    "#         id_tracker[fasta_id] = id_tracker.setdefault(fasta_id, 0) + 1\n",
    "#         fasta_id = f\"{fasta_id}_{id_tracker[fasta_id]}\"\n",
    "#         new_df.loc[new_df[\"acc_id\"] == uid, \"fasta_id\"] = fasta_id\n",
    "#         hdf_new.create_dataset(name=fasta_id, data=emb)\n",
    "\n",
    "# col_data = new_df.pop(\"fasta_id\")\n",
    "# new_df.insert(loc=0, column=\"fasta_id\", value=col_data)\n",
    "# new_df.to_csv(csv_, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dset-3ftx-zbvyu_fa-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f81b69f198023846c7ea0ce443045fb0d01be6197a356388c4bf1ce48e9cf6b6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
